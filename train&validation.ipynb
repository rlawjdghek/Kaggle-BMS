{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ------ LIBRARY -------#\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "#from rdkit import Chem\n",
    "#from rdkit import RDLogger\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import builtins\n",
    "import re\n",
    "import cv2\n",
    "# torch\n",
    "import timm\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, MultiStepLR, OneCycleLR\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from collections import defaultdict\n",
    "import itertools as it\n",
    "\n",
    "import tqdm\n",
    "\n",
    "#import time\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    debug=False\n",
    "    val_samples = 100 if debug  else 5_000 # 384 images -> 100_000\n",
    "    iter_log = 5000 # 1000\n",
    "    amp = True\n",
    "    gpu = '0'\n",
    "    #multi_gpu=True if len(gpu)>1 else False\n",
    "    #encoder='resnet101'#'[resnet34', 'tf_efficientnet_b0']\n",
    "    #decoder='unet'\n",
    "    \n",
    "    #epochs=2 \n",
    "    iteration = 80000 * 1000\n",
    "    batch_size=64\n",
    "    weight_decay=1e-6\n",
    "    n_fold=5\n",
    "    fold=3 # [0, 1, 2, 3, 4] # 원래는 3\n",
    "    #all_fold_train = False # all fold training\n",
    "    dir_ = f'./saved_models/768_decoder_1e-3/'#f'./saved_models/fold3_224_tnt/'\n",
    "    initial_checkpoint = './pretrained/00085000_2.3856_0.0000model.pth'#'./pretrained/decoder_448.pth'#'./data/deit_16_384.pth'#\n",
    "    #'./saved_models/scratch_fold3_224_tnt/checkpoint/2epoch_1.4170_2.1683_model.pth'\n",
    "    #'./saved_models/scratch_fold3_224_tnt/checkpoint/5epoch_1.8344model.pth'#'./pretrained/00298000_model.pth'\n",
    "    #encoder_checkpoint = None\n",
    "    start_lr = 1e-3#1e-3,5e-5\n",
    "    min_lr=1e-6\n",
    "    # ---- Dataset ---- #\n",
    "    #image_size = 448\n",
    "    vocab_size = 193\n",
    "    max_length = 300 #275\n",
    "    #image_path = f'./data/{image_size}x{image_size}/train'\n",
    "    \n",
    "\n",
    "    # -------- Encoder --------- #\n",
    "    #image_dim = 384#384 # 의미없음,, 바뀌는게 없음\n",
    "    patch_dim = 768\n",
    "    text_dim  = 768#384\n",
    "    #decoder_dim = 640#384\n",
    "    num_layer = 5 # 3\n",
    "    num_head = 8\n",
    "    ff_dim = 1024\n",
    "    #num_pixel=8*8 # when resnet101,256size 8x8, 224-> 7x7\n",
    "    \n",
    "    #------------ new ---------------#\n",
    "    patch_size = 16\n",
    "    pixel_pad    = 3\n",
    "    pixel_stride = 4\n",
    "    num_pixel    = 16#(patch_size // pixel_stride)**2\n",
    "    #pixel_scale = 0.4\n",
    "    max_patch_row_col = 500\n",
    "    max_num_patch = 600\n",
    "    \n",
    "    # 내맘대로 추가 dimension\n",
    "    #in_dim = 40 # 24\n",
    "    #num_heads = 10 #6\n",
    "    # ---- optimizer, scheduler .. ---- #\n",
    "    T_max=10 # CosineAnnealingLR\n",
    "    opt =  'radam_look' # [adamw, radam_look]\n",
    "    scheduler=None #'MultiStepLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    loss = 'bce' # [lovasz, bce, bce_dice, dice]\n",
    "    factor=0.35 # ReduceLROnPlateau, MultiStepLR\n",
    "    patience=3 # ReduceLROnPlateau\n",
    "    eps=1e-6 # ReduceLROnPlateau\n",
    "    \n",
    "    decay_epoch = [4, 8, 12, 16]\n",
    "    T_0=4 # CosineAnnealingWarmRestarts\n",
    "\n",
    "    \n",
    "    #----------------------------------#\n",
    "\n",
    "    # ---- Else ---- #\n",
    "    num_workers=16\n",
    "    seed=92\n",
    "\n",
    "STOI = {\n",
    "    '<sos>': 190,\n",
    "    '<eos>': 191,\n",
    "    '<pad>': 192,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "data_dir = './data/'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "##----------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True # for faster training, but not deterministic\n",
    "import random\n",
    "set_seeds(seed=args.seed)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     5,
     17,
     83,
     103,
     118,
     193,
     257,
     298,
     303
    ]
   },
   "outputs": [],
   "source": [
    "# ----- util ---- #\n",
    "\n",
    "#RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "#https://www.kaggle.com/nofreewill/normalize-your-predictions\n",
    "def normalize_inchi(inchi):\n",
    "    try:\n",
    "        mol = Chem.MolFromInchi(inchi)\n",
    "        if mol is not None:\n",
    "            try:\n",
    "                inchi = Chem.MolToInchi(mol)\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "    return inchi\n",
    "# -----------------------------------------------------------------------\n",
    "class YNakamaTokenizer(object):\n",
    "\n",
    "    def __init__(self, is_load=True):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "        if is_load:\n",
    "            self.stoi = read_pickle_from_file(data_dir+'/tokenizer.stoi.pickle')\n",
    "            self.itos = {k: v for v, k in self.stoi.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def build_vocab(self, text):\n",
    "        vocab = set()\n",
    "        for t in text:\n",
    "            vocab.update(t.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for i, s in enumerate(vocab):\n",
    "            self.stoi[s] = i\n",
    "        self.itos = {k: v for v, k in self.stoi.items()}\n",
    "\n",
    "    def one_text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "        sequence.append(self.stoi['<sos>'])\n",
    "        for s in text.split(' '):\n",
    "            sequence.append(self.stoi[s])\n",
    "        sequence.append(self.stoi['<eos>'])\n",
    "        return sequence\n",
    "\n",
    "    def one_sequence_to_text(self, sequence):\n",
    "        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n",
    "\n",
    "    def one_predict_to_inchi(self, predict):\n",
    "        inchi = 'InChI=1S/'\n",
    "        for p in predict:\n",
    "            if p == self.stoi['<eos>'] or p == self.stoi['<pad>']:\n",
    "                break\n",
    "            inchi += self.itos[p]\n",
    "        return inchi\n",
    "\n",
    "    # ---\n",
    "    def text_to_sequence(self, text):\n",
    "        sequence = [\n",
    "            self.one_text_to_sequence(t)\n",
    "            for t in text\n",
    "        ]\n",
    "        return sequence\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        text = [\n",
    "            self.one_sequence_to_text(s)\n",
    "            for s in sequence\n",
    "        ]\n",
    "        return text\n",
    "\n",
    "    def predict_to_inchi(self, predict):\n",
    "        inchi = [\n",
    "            self.one_predict_to_inchi(p)\n",
    "            for p in predict\n",
    "        ]\n",
    "        return inchi\n",
    "# -----------------------------------------------------------------------\n",
    "def compute_lb_score(predict, truth):\n",
    "    score = []\n",
    "    for p, t in zip(predict, truth):\n",
    "        s = Levenshtein.distance(p, t)\n",
    "        score.append(s)\n",
    "    score = np.array(score)\n",
    "    return score\n",
    "\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr +=[ param_group['lr'] ]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "# loss\n",
    "def np_loss_cross_entropy(probability, truth):\n",
    "    batch_size = len(probability)\n",
    "    truth = truth.reshape(-1)\n",
    "    p = probability[np.arange(batch_size),truth]\n",
    "    loss = -np.log(np.clip(p,1e-6,1))\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "#  https://www.kaggle.com/lextoumbourou/radampytorch#radam.py\n",
    "#  https://forums.fast.ai/t/meet-ranger-radam-lookahead-optimizer/52886/21\n",
    "#  https://github.com/nachiket273/lookahead_pytorch\n",
    "#  https://github.com/mgrankin/over9000\n",
    "\n",
    "\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value = 1 - beta2)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
    "                else:\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class PlainRAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "        super(PlainRAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(PlainRAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                exp_avg.mul_(beta1).add_( grad, alpha= 1 - beta1)\n",
    "\n",
    "                state['step'] += 1\n",
    "                beta2_t = beta2 ** state['step']\n",
    "                N_sma_max = 2 / (1 - beta2) - 1\n",
    "                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "                else:\n",
    "                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size )\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, alpha=0.5, k=6):\n",
    "\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        for group in self.param_groups:\n",
    "            group[\"step_counter\"] = 0\n",
    "\n",
    "        self.slow_weights = [\n",
    "                [p.clone().detach() for p in group['params']]\n",
    "            for group in self.param_groups]\n",
    "\n",
    "        for w in it.chain(*self.slow_weights):\n",
    "            w.requires_grad = False\n",
    "        self.state = optimizer.state\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        loss = self.optimizer.step()\n",
    "\n",
    "        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n",
    "            group['step_counter'] += 1\n",
    "            if group['step_counter'] % self.k != 0:\n",
    "                continue\n",
    "            for p,q in zip(group['params'],slow_weights):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                q.data.add_(p.data - q.data, alpha=self.alpha )\n",
    "                p.data.copy_(q.data)\n",
    "        return loss\n",
    "    \n",
    "#---------------------------------\n",
    "def compress_array(k):\n",
    "    compressed_k = io.BytesIO()\n",
    "    np.savez_compressed(compressed_k, k)\n",
    "    return compressed_k\n",
    "\n",
    "def uncompress_array(compressed_k):\n",
    "    compressed_k.seek(0)\n",
    "    k  = np.load(compressed_k,allow_pickle=True)['arr_0']\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     1,
     37,
     48,
     59,
     62,
     69,
     95,
     102,
     108,
     122,
     127,
     128,
     135,
     142,
     158,
     161,
     164,
     168
    ]
   },
   "outputs": [],
   "source": [
    "# ----- file.py -------#\n",
    "class Struct(object):\n",
    "    def __init__(self, is_copy=False, **kwargs):\n",
    "        self.add(is_copy, **kwargs)\n",
    "\n",
    "    def add(self, is_copy=False, **kwargs):\n",
    "        #self.__dict__.update(kwargs)\n",
    "\n",
    "        if is_copy == False:\n",
    "            for key, value in kwargs.items():\n",
    "                setattr(self, key, value)\n",
    "        else:\n",
    "            for key, value in kwargs.items():\n",
    "                try:\n",
    "                    setattr(self, key, copy.deepcopy(value))\n",
    "                    #setattr(self, key, value.copy())\n",
    "                except Exception:\n",
    "                    setattr(self, key, value)\n",
    "\n",
    "    def drop(self,  missing=None, **kwargs):\n",
    "        drop_value = []\n",
    "        for key, value in kwargs.items():\n",
    "            try:\n",
    "                delattr(self, key)\n",
    "                drop_value.append(value)\n",
    "            except:\n",
    "                drop_value.append(missing)\n",
    "        return drop_value\n",
    "\n",
    "    def __str__(self):\n",
    "        text =''\n",
    "        for k,v in self.__dict__.items():\n",
    "            text += '\\t%s : %s\\n'%(k, str(v))\n",
    "        return text\n",
    "\n",
    "\n",
    "# log ------------------------------------\n",
    "def remove_comments(lines, token='#'):\n",
    "    \"\"\" Generator. Strips comments and whitespace from input lines.\n",
    "    \"\"\"\n",
    "\n",
    "    l = []\n",
    "    for line in lines:\n",
    "        s = line.split(token, 1)[0].strip()\n",
    "        if s != '':\n",
    "            l.append(s)\n",
    "    return l\n",
    "\n",
    "def open(file, mode=None, encoding=None):\n",
    "    if mode == None: mode = 'r'\n",
    "\n",
    "    if '/' in file:\n",
    "        if 'w' or 'a' in mode:\n",
    "            dir = os.path.dirname(file)\n",
    "            if not os.path.isdir(dir):  os.makedirs(dir)\n",
    "\n",
    "    f = builtins.open(file, mode=mode, encoding=encoding)\n",
    "    return f\n",
    "\n",
    "def remove(file):\n",
    "    if os.path.exists(file): os.remove(file)\n",
    "\n",
    "def empty(dir):\n",
    "    if os.path.isdir(dir):\n",
    "        shutil.rmtree(dir, ignore_errors=True)\n",
    "    else:\n",
    "        os.makedirs(dir)\n",
    "\n",
    "# http://stackoverflow.com/questions/34950201/pycharm-print-end-r-statement-not-working\n",
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.terminal = sys.stdout  #stdout\n",
    "        self.file = None\n",
    "\n",
    "    def open(self, file, mode=None):\n",
    "        if mode is None: mode ='w'\n",
    "        self.file = open(file, mode)\n",
    "\n",
    "    def write(self, message, is_terminal=1, is_file=1 ):\n",
    "        if '\\r' in message: is_file=0\n",
    "\n",
    "        if is_terminal == 1:\n",
    "            self.terminal.write(message)\n",
    "            self.terminal.flush()\n",
    "            #time.sleep(1)\n",
    "\n",
    "        if is_file == 1:\n",
    "            self.file.write(message)\n",
    "            self.file.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "def print_args(args, logger=None):\n",
    "    for k, v in vars(args).items():\n",
    "        if logger is not None:\n",
    "            logger.write('{:<16} : {}\\n'.format(k, v))\n",
    "        else:\n",
    "            print('{:<16} : {}'.format(k, v))\n",
    "# io ------------------------------------\n",
    "def write_list_to_file(list_file, strings):\n",
    "    with open(list_file, 'w') as f:\n",
    "        for s in strings:\n",
    "            f.write('%s\\n'%str(s))\n",
    "    pass\n",
    "\n",
    "def read_list_from_file(list_file, comment='#'):\n",
    "    with open(list_file) as f:\n",
    "        lines  = f.readlines()\n",
    "    strings=[]\n",
    "    for line in lines:\n",
    "        if comment is not None:\n",
    "            s = line.split(comment, 1)[0].strip()\n",
    "        else:\n",
    "            s = line.strip()\n",
    "        if s != '':\n",
    "            strings.append(s)\n",
    "    return strings\n",
    "\n",
    "\n",
    "def read_pickle_from_file(pickle_file):\n",
    "    with open(pickle_file,'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    return x\n",
    "\n",
    "def write_pickle_to_file(pickle_file, x):\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(x, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# backup ------------------------------------\n",
    "\n",
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass\n",
    "\n",
    "# etc ------------------------------------\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def np_float32_to_uint8(x, scale=255):\n",
    "    return (x*scale).astype(np.uint8)\n",
    "\n",
    "def np_uint8_to_float32(x, scale=255):\n",
    "    return (x/scale).astype(np.float32)\n",
    "\n",
    "def int_tuple(x):\n",
    "    return tuple( [int(round(xx)) for xx in x] )\n",
    "\n",
    "\n",
    "def df_loc_by_list(df, key, values):\n",
    "    df = df.loc[df[key].isin(values)]\n",
    "    df = df.assign(sort = pd.Categorical(df[key], categories=values, ordered=True))\n",
    "    df = df.sort_values('sort')\n",
    "    #df = df.reset_index()\n",
    "    df = df.drop('sort', axis=1)\n",
    "    return  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     22,
     75,
     93,
     113,
     137,
     153,
     187,
     242
    ]
   },
   "outputs": [],
   "source": [
    "# ------- resize for DATASET ---------#\n",
    "#from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "\n",
    "def pad_sequence_to_max_length(sequence, max_length, padding_value):\n",
    "    batch_size =len(sequence)\n",
    "    pad_sequence = np.full((batch_size,max_length), padding_value, np.int32)\n",
    "    for b, s in enumerate(sequence):\n",
    "        L = len(s)\n",
    "        pad_sequence[b, :L, ...] = s\n",
    "    return pad_sequence\n",
    "\n",
    "def load_tokenizer():\n",
    "    tokenizer = YNakamaTokenizer(is_load=True)\n",
    "    print('len(tokenizer) : vocab_size', len(tokenizer))\n",
    "    for k,v in STOI.items():\n",
    "        assert  tokenizer.stoi[k]==v\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "#(2424186, 6)\n",
    "#Index(['image_id', 'InChI', 'formula', 'text', 'sequence', 'length'], dtype='object')\n",
    "def make_fold2(mode='train-1'):\n",
    "    if 'train' in mode:\n",
    "        df = read_pickle_from_file(data_dir+'/df_train.more.csv.pickle')\n",
    "        df_fold = pd.read_csv(data_dir+'/df_fold.csv')\n",
    "        df = df.merge(df_fold, on='image_id')\n",
    "        df.loc[:,'path']= args.image_path#'train_224'\n",
    "        df.loc[:, 'orientation'] = 0\n",
    "\n",
    "        df['fold'] = df['fold'].astype(int)\n",
    "        #print(df.groupby(['fold']).size()) #404_031\n",
    "        #print(df.columns)\n",
    "        if args.debug:\n",
    "            df = df.sample(n=3000, random_state=args.seed).reset_index(drop=True)\n",
    "        fold = int(mode.split('-')[-1])\n",
    "        df_train = df[df.fold != fold].reset_index(drop=True)\n",
    "        df_valid = df[df.fold == fold].reset_index(drop=True)\n",
    "        return df_train, df_valid\n",
    "\n",
    "    # Index(['image_id', 'InChI'], dtype='object')\n",
    "    if 'test' in mode:\n",
    "        df = pd.read_csv(data_dir+'/sample_submission.csv')\n",
    "        df_orientation = pd.read_csv(data_dir+'/test_orientation.csv')\n",
    "        df = df.merge(df_orientation, on='image_id')\n",
    "\n",
    "        df.loc[:, 'path'] = 'test_224'\n",
    "        df.loc[:, 'InChI'] = '0'\n",
    "        df.loc[:, 'formula'] = '0'\n",
    "        df.loc[:, 'text'] =  '0'\n",
    "        df.loc[:, 'sequence'] = pd.Series([[0]] * len(df))\n",
    "        df.loc[:, 'length'] = 1\n",
    "\n",
    "        df_test = df\n",
    "        return df_test\n",
    "def make_fold(mode='train-1'):\n",
    "    if 'train' in mode:\n",
    "        df = read_pickle_from_file(data_dir+'/df_train.more.csv.pickle')\n",
    "        #df_fold = pd.read_csv(data_dir+'/df_fold.csv')\n",
    "        df_fold = pd.read_csv(data_dir+'/df_fold.fine.csv')\n",
    "        df_meta = pd.read_csv(data_dir+'/df_train_image_meta.csv')\n",
    "        df = df.merge(df_fold, on='image_id')\n",
    "        df = df.merge(df_meta, on='image_id')\n",
    "        df.loc[:,'path']='train_patch16_s0.800'#train_patch16_s0.800 # train_patch16_s2.000\n",
    "\n",
    "        df['fold'] = df['fold'].astype(int)\n",
    "        #print(df.groupby(['fold']).size()) #404_031\n",
    "        #print(df.columns)\n",
    "\n",
    "        fold = int(mode[-1])*10\n",
    "        df_train = df[df.fold != fold].reset_index(drop=True)\n",
    "        df_valid = df[df.fold == fold].reset_index(drop=True)\n",
    "        return df_train, df_valid\n",
    "\n",
    "    # Index(['image_id', 'InChI'], dtype='object')\n",
    "    if 'test' in mode:\n",
    "        #df = pd.read_csv(data_dir+'/sample_submission.csv')\n",
    "        df = pd.read_csv(data_dir+'/submit_lb3.80.csv')\n",
    "        df_meta = pd.read_csv(data_dir+'/df_test_image_meta.csv')\n",
    "        df = df.merge(df_meta, on='image_id')\n",
    "\n",
    "        df.loc[:, 'path'] = 'test'\n",
    "        #df.loc[:, 'InChI'] = '0'\n",
    "        df.loc[:, 'formula'] = '0'\n",
    "        df.loc[:, 'text'] =  '0'\n",
    "        df.loc[:, 'sequence'] = pd.Series([[0]] * len(df))\n",
    "        df.loc[:, 'length'] = df.InChI.str.len()\n",
    "\n",
    "        df_test = df\n",
    "        return df_test\n",
    "\n",
    "#make_fold(mode='test')\n",
    "#####################################################################################################\n",
    "class FixNumSampler(Sampler):\n",
    "    def __init__(self, dataset, length=-1, is_shuffle=False):\n",
    "        if length<=0:\n",
    "            length=len(dataset)\n",
    "\n",
    "        self.is_shuffle = is_shuffle\n",
    "        self.length = length\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        index = np.arange(self.length)\n",
    "        if self.is_shuffle: random.shuffle(index)\n",
    "        return iter(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "\n",
    "# see https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-inference/data\n",
    "def remote_unrotate_augment(r):\n",
    "    image = r['image']\n",
    "    h, w = image.shape\n",
    "\n",
    "    # if h > w:\n",
    "    #     image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    l= r['d'].orientation\n",
    "    if l == 1:\n",
    "        image = np.rot90(image, -1)\n",
    "    if l == 2:\n",
    "        image = np.rot90(image, 1)\n",
    "    if l == 3:\n",
    "        image = np.rot90(image, 2)\n",
    "\n",
    "    #image = cv2.resize(image, dsize=(image_size,image_size), interpolation=cv2.INTER_LINEAR)\n",
    "    #assert args.image_size == 224\n",
    "\n",
    "    r['image'] = image\n",
    "    return r\n",
    "\n",
    "\n",
    "\n",
    "def null_augment(r):\n",
    "    return r\n",
    "def null_augment2(r):\n",
    "    image = r['image']\n",
    "    #image = cv2.resize(image, dsize=(image_size,image_size), interpolation=cv2.INTER_LINEAR)\n",
    "    #assert args.image_size==224\n",
    "    l = random.sample([1,2,3,4],1)[0]\n",
    "    if l == 1:\n",
    "        image = np.rot90(image, -1)\n",
    "    elif l == 2:\n",
    "        image = np.rot90(image, 1)\n",
    "    elif l == 3:\n",
    "        image = np.rot90(image, 2)\n",
    "    else:\n",
    "        pass\n",
    "    r['image'] = image\n",
    "    return r\n",
    "    \n",
    "class BmsDataset2(Dataset):\n",
    "    def __init__(self, df, tokenizer, augment=null_augment):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "        self.length = len(self.df)\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen = %d\\n'%len(self)\n",
    "        string += '\\tdf  = %s\\n'%str(self.df.shape)\n",
    "        return string\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.df.iloc[index]\n",
    "\n",
    "        image_file = f'{d.path}/{d.image_id}.png'\n",
    "        image = cv2.imread(image_file,cv2.IMREAD_GRAYSCALE)\n",
    "        token = d.sequence\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'image_id' : d.image_id,\n",
    "            'InChI' : d.InChI,\n",
    "            'formula' : d.formula,\n",
    "            'd' : d,\n",
    "            'image' : image,\n",
    "            'token' : token,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        return r\n",
    "class BmsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, augment=null_augment):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "        self.length = len(self.df)\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen = %d\\n'%len(self)\n",
    "        string += '\\tdf  = %s\\n'%str(self.df.shape)\n",
    "\n",
    "        g = self.df['length'].values.astype(np.int32)//20\n",
    "        g = np.bincount(g,minlength=14)\n",
    "        string += '\\tlength distribution\\n'\n",
    "        for n in range(14):\n",
    "            string += '\\t\\t %3d = %8d (%0.4f)\\n'%((n+1)*20,g[n], g[n]/g.sum() )\n",
    "        return string\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.df.iloc[index]\n",
    "        token = d.sequence\n",
    "\n",
    "        patch_file = data_dir +'/%s/%s/%s/%s/%s.pickle'%(d.path, d.image_id[0], d.image_id[1], d.image_id[2], d.image_id)\n",
    "        k = read_pickle_from_file(patch_file)\n",
    "\n",
    "        patch = uncompress_array(k['patch'])\n",
    "        patch = np.concatenate([\n",
    "            np.zeros((1, args.patch_size+2*args.pixel_pad, args.patch_size+2*args.pixel_pad), np.uint8),\n",
    "            patch],0) #cls token\n",
    "\n",
    "        coord  = k['coord']\n",
    "        w = k['width' ]\n",
    "        h = k['height']\n",
    "\n",
    "        h = h // args.patch_size -1\n",
    "        w = w // args.patch_size -1\n",
    "        coord = np.insert(coord, 0, [h, w], 0) #cls token\n",
    "\n",
    "        r = {\n",
    "            'index'    : index,\n",
    "            'image_id' : d.image_id,\n",
    "            'InChI'    : d.InChI,\n",
    "            'd' : d,\n",
    "            'token' : token,\n",
    "            'patch' : patch,\n",
    "            'coord' : coord,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        return r\n",
    "\n",
    "def null_collate(batch, is_sort_decreasing_length=True):\n",
    "    collate = defaultdict(list)\n",
    "\n",
    "    if is_sort_decreasing_length: #sort by decreasing length\n",
    "        sort  = np.argsort([-len(r['token']) for r in batch])\n",
    "        batch = [batch[s] for s in sort]\n",
    "\n",
    "    for r in batch:\n",
    "        for k, v in r.items():\n",
    "            collate[k].append(v)\n",
    "    #----\n",
    "\n",
    "    batch_size = len(batch)\n",
    "    collate['length'] = [len(l) for l in collate['token']]\n",
    "\n",
    "    token  = [np.array(t,np.int32) for t in collate['token']]\n",
    "    token  = pad_sequence_to_max_length(token, max_length=args.max_length, padding_value=STOI['<pad>'])\n",
    "    collate['token'] = torch.from_numpy(token).long()\n",
    "\n",
    "    max_of_length = max(collate['length'])\n",
    "    token_pad_mask  = np.zeros((batch_size, max_of_length, max_of_length))\n",
    "    for b in range(batch_size):\n",
    "        L = collate['length'][b]\n",
    "        token_pad_mask [b, :L, :L] = 1 #+1 for cls_token\n",
    "\n",
    "    collate['token_pad_mask'] = torch.from_numpy(token_pad_mask).byte()\n",
    "    #-----\n",
    "    # image = np.stack(collate['image'])\n",
    "    # image = image.astype(np.float32) / 255\n",
    "    # collate['image'] = torch.from_numpy(image).unsqueeze(1).repeat(1,3,1,1)\n",
    "\n",
    "    #-----\n",
    "\n",
    "    collate['num_patch'] = [len(l) for l in collate['patch']]\n",
    "\n",
    "    max_of_num_patch = max(collate['num_patch'])\n",
    "    patch_pad_mask  = np.zeros((batch_size, max_of_num_patch, max_of_num_patch))\n",
    "    patch = np.full((batch_size, max_of_num_patch, args.patch_size+2*args.pixel_pad, args.patch_size+2*args.pixel_pad),255) #pad as 255\n",
    "    coord = np.zeros((batch_size, max_of_num_patch, 2))\n",
    "    for b in range(batch_size):\n",
    "        N = collate['num_patch'][b]\n",
    "        patch[b, :N] = collate['patch'][b]\n",
    "        coord[b, :N] = collate['coord'][b]\n",
    "        patch_pad_mask [b, :N, :N] = 1 #+1 for cls_token\n",
    "\n",
    "    collate['patch'] = torch.from_numpy(patch).half() / 255\n",
    "    collate['coord'] = torch.from_numpy(coord).long()\n",
    "    collate['patch_pad_mask' ] = torch.from_numpy(patch_pad_mask).byte()\n",
    "    return collate\n",
    "\n",
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bms dataset고쳐야함'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"bms dataset고쳐야함\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TNT버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     6,
     24,
     62,
     82
    ]
   },
   "outputs": [],
   "source": [
    "# vit\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "#-------------------------------------------------\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self,\n",
    "        x: Tensor,\n",
    "        mask: Optional[Tensor] = None\n",
    "    )-> Tensor:\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale # B x self.num_heads x NxN\n",
    "        if mask is not None:\n",
    "            #mask = mask.unsqueeze(1).repeat(1,self.num_heads,1,1)\n",
    "            mask = mask.unsqueeze(1).expand(-1,self.num_heads,-1,-1)\n",
    "            attn = attn.masked_fill(mask == 0, -6e4)\n",
    "            # attn = attn.masked_fill(mask == 0, -half('inf'))\n",
    "            # https://github.com/NVIDIA/apex/issues/93\n",
    "            # How to use fp16 training with masked operations\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x),mask))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=1, embed_dim=768, patch_size=16, norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    # url='https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth',\n",
    "    # vit_deit_base_distilled_patch16_384\n",
    "    def __init__(self,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.,\n",
    "        attn_drop_rate=0.,\n",
    "        drop_path_rate=0.,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer  = nn.GELU\n",
    "\n",
    "\n",
    "        self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.patch_embed = PatchEmbed( patch_size=args.patch_size, embed_dim=embed_dim)\n",
    "\n",
    "        #self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Embedding(args.max_patch_row_col*args.max_patch_row_col, embed_dim)\n",
    "        self.pos_drop  = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        # if representation_size and not distilled:\n",
    "        #     self.num_features = representation_size\n",
    "        #     self.pre_logits = nn.Sequential(OrderedDict([\n",
    "        #         ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "        #         ('act', nn.Tanh())\n",
    "        #     ]))\n",
    "        # else:\n",
    "        #     self.pre_logits = nn.Identity()\n",
    "        trunc_normal_(self.pos_embed.weight, std=.02)\n",
    "\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    #todo: ensure not decay in optimizer\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "\n",
    "    def forward(self, patch, coord, patch_pad_mask):\n",
    "        max_patch_row_col = args.max_patch_row_col\n",
    "        batch_size, max_of_num_patch, patch_size, patch_size = patch.shape\n",
    "        x = patch.reshape(batch_size*max_of_num_patch, 1, patch_size, patch_size)\n",
    "\n",
    "        x = self.patch_embed(x).reshape(batch_size,max_of_num_patch,-1)\n",
    "        #cls_token = self.cls_token.expand(batch_size,max_of_num_patch, -1)\n",
    "        #x = torch.cat((cls_token, x), dim=1) #cls token is already in patch\n",
    "        x = x + self.pos_embed(coord[:, :, 0] * max_patch_row_col + coord[:, :, 1])\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x,patch_pad_mask)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     16,
     33,
     60
    ]
   },
   "outputs": [],
   "source": [
    "# fairseq #2\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "from fairseq import utils\n",
    "from fairseq.models import *\n",
    "from fairseq.modules import *\n",
    "\n",
    "#https://stackoverflow.com/questions/4984647/accessing-dict-keys-like-an-attribute\n",
    "class Namespace(object):\n",
    "    def __init__(self, adict):\n",
    "        self.__dict__.update(adict)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
    "# https://stackoverflow.com/questions/46452020/sinusoidal-embedding-attention-is-all-you-need\n",
    "\n",
    "class PositionEncode1D(nn.Module):\n",
    "    def __init__(self, dim, max_length):\n",
    "        super().__init__()\n",
    "        assert (dim % 2 == 0)\n",
    "        self.max_length = max_length\n",
    "\n",
    "        d = torch.exp(torch.arange(0., dim, 2)* (-math.log(10000.0) / dim))\n",
    "        position = torch.arange(0., max_length).unsqueeze(1)\n",
    "        pos = torch.zeros(1, max_length, dim)\n",
    "        pos[0, :, 0::2] = torch.sin(position * d)\n",
    "        pos[0, :, 1::2] = torch.cos(position * d)\n",
    "        self.register_buffer('pos', pos)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, T, dim = x.shape\n",
    "        x = x + self.pos[:,:T]\n",
    "        return x\n",
    "    \n",
    "class TransformerEncode(FairseqEncoder):\n",
    "\n",
    "    def __init__(self, dim, ff_dim, num_head, num_layer):\n",
    "        super().__init__({})\n",
    "        #print('my TransformerEncode()')\n",
    "\n",
    "        self.layer = nn.ModuleList([\n",
    "            TransformerEncoderLayer(Namespace({\n",
    "                'encoder_embed_dim': dim,\n",
    "                'encoder_attention_heads': num_head,\n",
    "                'attention_dropout': 0.1,\n",
    "                'dropout': 0.1,\n",
    "                'encoder_normalize_before': True,\n",
    "                'encoder_ffn_embed_dim': ff_dim,\n",
    "            })) for i in range(num_layer)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):# T x B x C\n",
    "        #print('my TransformerEncode forward()')\n",
    "        for layer in self.layer:\n",
    "            x = layer(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "# https://fairseq.readthedocs.io/en/latest/tutorial_simple_lstm.html\n",
    "# see https://gitlab.maastrichtuniversity.nl/dsri-examples/dsri-pytorch-workspace/-/blob/c8a88cdeb8e1a0f3a2ccd3c6119f43743cbb01e9/examples/transformer/fairseq/models/transformer.py\n",
    "class TransformerDecode(FairseqIncrementalDecoder):\n",
    "    def __init__(self, encoder_dim, decoder_dim, ff_dim, num_head, num_layer):\n",
    "        super().__init__({})\n",
    "        #print('my TransformerDecode()')\n",
    "\n",
    "        self.layer = nn.ModuleList([\n",
    "            TransformerDecoderLayer(Namespace({\n",
    "                'encoder_embed_dim': encoder_dim,\n",
    "                'decoder_embed_dim': decoder_dim,\n",
    "                'decoder_attention_heads': num_head,\n",
    "                'attention_dropout': 0.1,\n",
    "                'dropout': 0.1,\n",
    "                'decoder_normalize_before': True,\n",
    "                'decoder_ffn_embed_dim': ff_dim,\n",
    "            })) for i in range(num_layer)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(decoder_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, mem, x_mask, x_pad_mask, mem_pad_mask):\n",
    "            #print('my TransformerDecode forward()')\n",
    "            for layer in self.layer:\n",
    "                x = layer(\n",
    "                    x,\n",
    "                    mem,\n",
    "                    self_attn_mask=x_mask,\n",
    "                    self_attn_padding_mask=x_pad_mask,\n",
    "                    encoder_padding_mask=mem_pad_mask,\n",
    "                )[0]\n",
    "            x = self.layer_norm(x)\n",
    "            return x  # T x B x C\n",
    "\n",
    "    #def forward_one(self, x, mem, incremental_state):\n",
    "    def forward_one(self,\n",
    "            x   : Tensor,\n",
    "            mem : Tensor,\n",
    "            mem_pad_mask : Tensor,\n",
    "            incremental_state : Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n",
    "    )-> Tensor:\n",
    "        x = x[-1:]\n",
    "        for layer in self.layer:\n",
    "            x = layer(\n",
    "                x,\n",
    "                mem,\n",
    "                encoder_padding_mask=mem_pad_mask,\n",
    "                incremental_state=incremental_state\n",
    "            )[0]\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     4,
     126,
     135
    ]
   },
   "outputs": [],
   "source": [
    "# NET #2\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn = VisionTransformer()\n",
    "        self.image_encode = nn.Identity()\n",
    "\n",
    "        #---\n",
    "        self.text_pos    = PositionEncode1D(args.text_dim,args.max_length)\n",
    "        self.token_embed = nn.Embedding(args.vocab_size, args.text_dim)\n",
    "        self.text_decode = TransformerDecode(args.patch_dim, args.text_dim, args.ff_dim, args.num_head, args.num_layer)\n",
    "\n",
    "        #---\n",
    "        self.logit  = nn.Linear(args.text_dim, args.vocab_size)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        #----\n",
    "        # initialization\n",
    "        self.token_embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.logit.bias.data.fill_(0)\n",
    "        self.logit.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def forward(self, patch, coord, token, patch_pad_mask, token_pad_mask):\n",
    "        device = patch.device\n",
    "        batch_size = len(patch)\n",
    "        #---\n",
    "        patch = patch*2-1\n",
    "        image_embed = self.cnn(patch, coord, patch_pad_mask)\n",
    "        image_embed = self.image_encode(image_embed).permute(1,0,2).contiguous()\n",
    "\n",
    "        text_embed = self.token_embed(token)\n",
    "        text_embed = self.text_pos(text_embed).permute(1,0,2).contiguous()\n",
    "\n",
    "        max_of_length = token_pad_mask.shape[-1]\n",
    "        text_mask = np.triu(np.ones((max_of_length, max_of_length)), k=1).astype(np.uint8)\n",
    "        text_mask = torch.autograd.Variable(torch.from_numpy(text_mask)==1).to(device)\n",
    "\n",
    "        #----\n",
    "        # <todo> perturb mask as aug\n",
    "        text_pad_mask = token_pad_mask[:,:,0]==0\n",
    "        image_pad_mask = patch_pad_mask[:,:,0]==0\n",
    "        x = self.text_decode(text_embed[:max_of_length], image_embed, text_mask, text_pad_mask, image_pad_mask)\n",
    "        x = x.permute(1,0,2).contiguous()\n",
    "        l = self.logit(x)\n",
    "\n",
    "        logit = torch.zeros((batch_size, args.max_length, args.vocab_size),device=device)\n",
    "        logit[:,:max_of_length]=l\n",
    "        return logit\n",
    "\n",
    "    @torch.jit.export\n",
    "    def forward_argmax_decode(self, patch, coord, patch_pad_mask):\n",
    "\n",
    "        patch_dim = args.patch_dim\n",
    "        text_dim  = args.text_dim\n",
    "        num_layer = args.num_layer\n",
    "        num_head  = args.num_head\n",
    "        ff_dim    = args.ff_dim\n",
    "        STOI = {\n",
    "            '<sos>': 190,\n",
    "            '<eos>': 191,\n",
    "            '<pad>': 192,\n",
    "        }\n",
    "        max_length = args.max_length # 275\n",
    "\n",
    "\n",
    "        #---------------------------------\n",
    "        device = patch.device\n",
    "        batch_size = len(patch)\n",
    "\n",
    "        #todo: test coord larger then train\n",
    "        coord = coord.clamp_max(99)\n",
    "\n",
    "        patch = patch*2-1\n",
    "        image_embed = self.cnn(patch, coord, patch_pad_mask)\n",
    "        image_embed = self.image_encode(image_embed).permute(1,0,2).contiguous()\n",
    "\n",
    "        token = torch.full((batch_size, max_length), STOI['<pad>'],dtype=torch.long, device=device)\n",
    "        text_pos = self.text_pos.pos\n",
    "        token[:,0] = STOI['<sos>']\n",
    "\n",
    "\n",
    "        #-------------------------------------\n",
    "        eos = STOI['<eos>']\n",
    "        pad = STOI['<pad>']\n",
    "\n",
    "\n",
    "        # fast version\n",
    "        if 1:\n",
    "            #incremental_state = {}\n",
    "            incremental_state = torch.jit.annotate(\n",
    "                Dict[str, Dict[str, Optional[Tensor]]],\n",
    "                torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}),\n",
    "            )\n",
    "            image_pad_mask = patch_pad_mask[:,:,0]==0\n",
    "            for t in range(max_length-1):\n",
    "                #last_token = token [:,:(t+1)]\n",
    "                #text_embed = self.token_embed(last_token)\n",
    "                #text_embed = self.text_pos(text_embed) #text_embed + text_pos[:,:(t+1)] #\n",
    "\n",
    "                last_token = token[:, t]\n",
    "                text_embed = self.token_embed(last_token)\n",
    "                text_embed = text_embed + text_pos[:,t] #\n",
    "                text_embed = text_embed.reshape(1,batch_size,text_dim)\n",
    "\n",
    "                x = self.text_decode.forward_one(text_embed, image_embed, image_pad_mask, incremental_state )\n",
    "                x = x.reshape(batch_size,text_dim)\n",
    "                #print(incremental_state.keys())\n",
    "\n",
    "                l = self.logit(x)\n",
    "                k = torch.argmax(l, -1)  # predict max\n",
    "                token[:, t+1] = k\n",
    "                if ((k == eos) | (k == pad)).all():  break\n",
    "\n",
    "        predict = token[:, 1:]\n",
    "        return predict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loss #################################################################\n",
    "def seq_cross_entropy_loss(logit, token, length):\n",
    "    truth = token[:, 1:]\n",
    "    L = [l - 1 for l in length]\n",
    "    logit = pack_padded_sequence(logit, L, batch_first=True).data\n",
    "    truth = pack_padded_sequence(truth, L, batch_first=True).data\n",
    "    loss = F.cross_entropy(logit, truth, ignore_index=STOI['<pad>'])\n",
    "    return loss\n",
    "\n",
    "# https://www.aclweb.org/anthology/2020.findings-emnlp.276.pdf\n",
    "def seq_focal_cross_entropy_loss(logit, token, length):\n",
    "    gamma = 0.5 # {0.5,1.0}\n",
    "    #label_smooth = 0.90\n",
    "\n",
    "    #---\n",
    "    truth = token[:, 1:]\n",
    "    L = [l - 1 for l in length]\n",
    "    logit = pack_padded_sequence(logit, L, batch_first=True).data\n",
    "    truth = pack_padded_sequence(truth, L, batch_first=True).data\n",
    "    #loss = F.cross_entropy(logit, truth, ignore_index=STOI['<pad>'])\n",
    "    #non_pad = torch.where(truth != STOI['<pad>'])[0]  # & (t!=STOI['<sos>'])\n",
    "\n",
    "\n",
    "    # ---\n",
    "    #p = F.softmax(logit,-1)\n",
    "    #logp = - torch.log(torch.clamp(p, 1e-4, 1 - 1e-4))\n",
    "\n",
    "    logp = F.log_softmax(logit, -1)\n",
    "    logp = logp.gather(1, truth.reshape(-1,1)).reshape(-1)\n",
    "    p = logp.exp()\n",
    "\n",
    "    loss = - ((1 - p) ** gamma)*logp  #focal\n",
    "    #loss = - ((1 + p) ** gamma)*logp  #anti-focal\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     66
    ]
   },
   "outputs": [],
   "source": [
    "def do_valid(net, tokenizer, valid_loader):\n",
    "\n",
    "    valid_probability = []\n",
    "    valid_truth = []\n",
    "    valid_length = []\n",
    "    valid_num = 0\n",
    "\n",
    "    net.eval()\n",
    "    start_timer = timer()\n",
    "    for t, batch in enumerate(valid_loader):\n",
    "        \n",
    "        batch_size = len(batch['index'])\n",
    "        length = batch['length']\n",
    "        token  = batch['token' ].cuda()\n",
    "        token_pad_mask = batch['token_pad_mask' ].cuda()\n",
    "        #image  = batch['image' ].cuda()\n",
    "        num_patch = batch['num_patch']\n",
    "        patch  = batch['patch' ].cuda()\n",
    "        coord  = batch['coord' ].cuda()\n",
    "        patch_pad_mask  = batch['patch_pad_mask' ].cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with amp.autocast():\n",
    "                logit = data_parallel(net,(patch, coord, token, patch_pad_mask, token_pad_mask)) #net(image, token, length)\n",
    "                probability = F.softmax(logit,-1)\n",
    "\n",
    "        valid_num += batch_size\n",
    "        valid_probability.append(probability.data.cpu().numpy())\n",
    "        valid_truth.append(token.data.cpu().numpy())\n",
    "        valid_length.extend(length)\n",
    "        print('\\r %8d / %d  %s'%(valid_num, len(valid_loader.sampler),time_to_str(timer() - start_timer,'sec')),end='',flush=True)\n",
    "\n",
    "    assert(valid_num == len(valid_loader.sampler)) #len(valid_loader.dataset))\n",
    "    #print('')\n",
    "    #----------------------\n",
    "    probability = np.concatenate(valid_probability)\n",
    "    predict = probability.argmax(-1)\n",
    "    truth   = np.concatenate(valid_truth)\n",
    "    length  = valid_length\n",
    "\n",
    "\n",
    "    #----\n",
    "    p = probability[:,:-1].reshape(-1, args.vocab_size)\n",
    "    t = truth[:,1:].reshape(-1)\n",
    "\n",
    "    non_pad = np.where(t!=STOI['<pad>'])[0] #& (t!=STOI['<sos>'])\n",
    "    p = p[non_pad]\n",
    "    t = t[non_pad]\n",
    "    loss = np_loss_cross_entropy(p, t)\n",
    "\n",
    "    #----\n",
    "    lb_score = 0\n",
    "    if 1:\n",
    "        score = []\n",
    "        for i, (p, t) in enumerate(zip(predict, truth)):\n",
    "            t = truth[i][1:length[i]-1]     # in the buggy version, i have used 1 instead of i\n",
    "            p = predict[i][1:length[i]-1]\n",
    "            t = tokenizer.one_predict_to_inchi(t)\n",
    "            p = tokenizer.one_predict_to_inchi(p)\n",
    "            s = Levenshtein.distance(p, t)\n",
    "            score.append(s)\n",
    "        lb_score = np.mean(score)\n",
    "\n",
    "    #lb_score = compute_lb_score(k, t)\n",
    "    return [loss, lb_score]\n",
    "\n",
    "def do_predict(net, tokenizer, valid_loader):\n",
    "\n",
    "    text = []\n",
    "    tokens = []\n",
    "    inchis = []\n",
    "    start_timer = timer()\n",
    "    valid_num = 0\n",
    "    for t, batch in enumerate(valid_loader):\n",
    "        batch_size = len(batch['index'])\n",
    "        inchi = batch['InChI']\n",
    "        patch  = batch['patch' ].cuda()\n",
    "        coord  = batch['coord' ].cuda()\n",
    "        patch_pad_mask  = batch['patch_pad_mask' ].cuda()\n",
    "        \n",
    "        inchis.extend(inchi)\n",
    "        #token  = batch['token']\n",
    "        #tokens.extend(token)\n",
    "\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            with amp.autocast():\n",
    "                k = net.forward_argmax_decode(patch, coord, patch_pad_mask)\n",
    "\n",
    "\n",
    "                k = k.data.cpu().numpy()\n",
    "                k = tokenizer.predict_to_inchi(k)\n",
    "                text.extend(k)\n",
    "\n",
    "        valid_num += batch_size\n",
    "        print('\\r %8d / %d  %s' % (valid_num, len(valid_loader.sampler), time_to_str(timer() - start_timer, 'sec')),\n",
    "              end='', flush=True)\n",
    "\n",
    "    assert(valid_num == len(valid_loader.sampler))\n",
    "    print('')\n",
    "    return text, inchis\n",
    "\n",
    "def run_train():\n",
    "    out_dir = args.dir_\n",
    "\n",
    "\n",
    "    ## setup  ----------------------------------------\n",
    "    for f in ['checkpoint', 'train', 'valid', 'backup']: os.makedirs(out_dir + '/' + f, exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir + '/log.train.txt', mode='a')\n",
    "    print_args(args, log)\n",
    "    log.write('\\n')\n",
    "    \n",
    "    \n",
    "    #\n",
    "    for n_fold in range(5):\n",
    "        if n_fold != args.fold:\n",
    "            print(f'{n_fold} fold pass')\n",
    "            continue\n",
    "    \n",
    "        ## dataset ------------------------------------\n",
    "        df_train, df_valid = make_fold('train-%d' % n_fold)\n",
    "\n",
    "        tokenizer = load_tokenizer()\n",
    "        train_dataset = BmsDataset(df_train,tokenizer)#, augment=null_augment2)\n",
    "        valid_dataset = BmsDataset(df_valid,tokenizer)#, augment=null_augment)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size=args.batch_size,\n",
    "            drop_last=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n",
    "            collate_fn=null_collate,\n",
    "        )\n",
    "        \n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            #sampler=SequentialSampler(valid_dataset),\n",
    "            sampler=FixNumSampler(valid_dataset, args.val_samples), #200_000 #5_000\n",
    "            batch_size=args.batch_size,\n",
    "            drop_last=False,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            collate_fn=null_collate,\n",
    "            #collate_fn=lambda batch: null_collate(batch,False),\n",
    "        )\n",
    "\n",
    "        log.write('** dataset setting **\\n')\n",
    "        log.write('train_dataset : \\n%s\\n' % (train_dataset))\n",
    "        log.write('valid_dataset : \\n%s\\n' % (valid_dataset))\n",
    "        log.write('\\n')\n",
    "\n",
    "\n",
    "        ## net ----------------------------------------\n",
    "        scaler = amp.GradScaler()\n",
    "        net = Net().to(device)\n",
    "\n",
    "        if args.initial_checkpoint is not None:\n",
    "#             temp = torch.load(args.initial_checkpoint)['state_dict']\n",
    "#             net.load_state_dict(temp)\n",
    "\n",
    "            # 여기는 처음 시작할때\n",
    "            temp = torch.load(args.initial_checkpoint)['state_dict']\n",
    "            for i in temp.copy().keys():\n",
    "                if 'cnn' not in i:\n",
    "                    del temp[i]\n",
    "#             for kk in range(3):\n",
    "#                 del temp[f'text_decode.layer.{kk}.encoder_attn.k_proj.weight']\n",
    "#                 del temp[f'text_decode.layer.{kk}.encoder_attn.v_proj.weight']\n",
    "            net.load_state_dict(temp, strict=False)\n",
    "\n",
    "        else:\n",
    "            start_iteration = 0\n",
    "            start_epoch = 0\n",
    "\n",
    "        log.write('** net setting **\\n')\n",
    "        log.write('\\n')\n",
    "        # -----------------------------------------------\n",
    "        if 0:  ##freeze\n",
    "            for p in net.encoder.parameters(): p.requires_grad = False\n",
    "\n",
    "        # ------------------------\n",
    "        #  Optimizer\n",
    "        # ------------------------\n",
    "        optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad,net.parameters()), lr=args.start_lr), alpha=0.5, k=5)\n",
    "\n",
    "        # ------------------------\n",
    "        #  scheduler\n",
    "        # ------------------------\n",
    "        def get_scheduler(optimizer):\n",
    "            if args.scheduler=='ReduceLROnPlateau':\n",
    "                scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=args.factor, patience=args.patience, \n",
    "                                              min_lr = 1e-5, verbose=True, eps=args.eps)\n",
    "            elif args.scheduler=='CosineAnnealingLR':\n",
    "                print('scheduler : Cosineannealinglr')\n",
    "                scheduler = CosineAnnealingLR(optimizer, T_max=args.T_max, eta_min=args.min_lr, last_epoch=-1)\n",
    "            elif args.scheduler=='CosineAnnealingWarmRestarts':\n",
    "                scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=args.T_0, T_mult=1, eta_min=args.min_lr, last_epoch=-1)\n",
    "            elif args.scheduler == 'MultiStepLR':\n",
    "                scheduler = MultiStepLR(optimizer, milestones=args.decay_epoch, gamma= args.factor, verbose=True)\n",
    "            elif args.scheduler == 'OneCycleLR':\n",
    "                scheduler = OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=1e-3, epochs=args.epochs, steps_per_epoch=len(train_loader))\n",
    "            else:\n",
    "                scheduler = None\n",
    "                log.write('scheduler is None')\n",
    "            return scheduler\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "\n",
    "        ## start training here! ##############################################\n",
    "        log.write('** start training here! **\\n')\n",
    "        log.write('                    |----- VALID -----------|---- TRAIN/BATCH --------------\\n')\n",
    "        log.write('rate    iter  epoch | loss  lb(tfO) lb(tfX) | loss0  loss1  | time          \\n')\n",
    "        log.write('----------------------------------------------------------------------------\\n')\n",
    "                 # 0.00000   0.00* 0.00  | 0.000  0.000  | 0.000  0.000  |  0 hr 00 min\n",
    "\n",
    "        num_iteration = args.iteration\n",
    "        iter_log = args.iter_log#2000\n",
    "        iter_valid = args.iter_log#2000\n",
    "        iter_save = list(range(0, num_iteration, args.iter_log))  # 1*1000\n",
    "        def message(mode='print'):\n",
    "            if mode == ('print'):\n",
    "                asterisk = ' '\n",
    "                loss = batch_loss\n",
    "            if mode == ('log'):\n",
    "                asterisk = '*' if iteration in iter_save else ' '\n",
    "                loss = train_loss\n",
    "\n",
    "            text = \\\n",
    "                '%0.5f  %5.4f%s %4.2f  | ' % (rate, iteration / 10000, asterisk, epoch,) + \\\n",
    "                '%4.3f  %4.3f  %4.3f   | ' % (valid_loss[0],valid_loss[1],tfX_score) + \\\n",
    "                '%4.3f  %4.3f  %4.3f   | ' % (*loss,) + \\\n",
    "                '%s' % (time_to_str(timer() - start_timer, 'min'))\n",
    "\n",
    "            return text\n",
    "\n",
    "        # ----\n",
    "        valid_loss = np.zeros(2, np.float32)\n",
    "        train_loss = np.zeros(3, np.float32)\n",
    "        batch_loss = np.zeros_like(train_loss)\n",
    "        sum_train_loss = np.zeros_like(train_loss)\n",
    "        sum_train = 0\n",
    "        loss0 = torch.FloatTensor([0]).cuda().sum()\n",
    "        loss1 = torch.FloatTensor([0]).cuda().sum()\n",
    "        loss2 = torch.FloatTensor([0]).cuda().sum()\n",
    "\n",
    "        start_timer = timer()\n",
    "        start_iteration = 0 ; start_epoch = 0 ; sch_epoch = 0\n",
    "        rate = 0\n",
    "        iteration = start_iteration\n",
    "        epoch = start_epoch\n",
    "        best_lb = np.inf\n",
    "        #for epoch in range(1, args.epochs+1):\n",
    "        while iteration < num_iteration:\n",
    "            for t, batch in enumerate(tqdm.tqdm(train_loader)):\n",
    "                \n",
    "                \n",
    "                if iteration in iter_save:\n",
    "                    if iteration != start_iteration:\n",
    "                        torch.save({\n",
    "                            'state_dict': net.state_dict(),\n",
    "                            'iteration': iteration,\n",
    "                            'epoch': epoch,\n",
    "                        }, out_dir + '/checkpoint/%08d_%.4f_%.4fmodel.pth' % (iteration, valid_loss[1], tfX_score ))\n",
    "                    \n",
    "                # learning rate schduler ------------\n",
    "                rate = get_learning_rate(optimizer)\n",
    "\n",
    "                # one iteration update  -------------                \n",
    "                batch_size = len(batch['index'])\n",
    "                length = batch['length']\n",
    "                token  = batch['token' ].cuda()\n",
    "                token_pad_mask = batch['token_pad_mask' ].cuda()\n",
    "                #image  = batch['image' ].cuda()\n",
    "                num_patch = batch['num_patch']\n",
    "                patch  = batch['patch' ].cuda()\n",
    "                coord  = batch['coord' ].cuda()\n",
    "                patch_pad_mask = batch['patch_pad_mask' ].cuda()\n",
    "                \n",
    "\n",
    "                # ----\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                \n",
    "                if args.amp:\n",
    "                    with amp.autocast():\n",
    "                        #assert(False)\n",
    "                        #logit = data_parallel(net, (image, token, length))\n",
    "                        logit = data_parallel(net, (patch, coord, token, patch_pad_mask, token_pad_mask))\n",
    "                        \n",
    "                        loss0 = seq_cross_entropy_loss(logit, token, length)\n",
    "\n",
    "\n",
    "\n",
    "                    scaler.scale(loss0).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                \n",
    "                # print statistics  --------\n",
    "                epoch += 1 / len(train_loader)\n",
    "                iteration += 1\n",
    "                \n",
    "\n",
    "                \n",
    "\n",
    "                batch_loss = np.array([loss0.item(), loss1.item(), loss2.item()])\n",
    "                sum_train_loss += batch_loss\n",
    "                sum_train += 1\n",
    "            \n",
    "                if iteration % 100 == 0:\n",
    "                    train_loss = sum_train_loss / (sum_train + 1e-12)\n",
    "                    sum_train_loss[...] = 0\n",
    "                    sum_train = 0\n",
    "\n",
    "\n",
    "                # validation\n",
    "                if (iteration % iter_valid == 0):\n",
    "                    valid_loss = do_valid(net, tokenizer, valid_loader)\n",
    "                    tfX_score = 0.\n",
    "                    if (iteration % (iter_valid*2) == 0):\n",
    "                        predict_, truth_ = do_predict(net, tokenizer, valid_loader)\n",
    "                        tfX_score = compute_lb_score(predict_, truth_).mean()\n",
    "                if (iteration % iter_log == 0):\n",
    "                    log.write('\\n'+message(mode='log') + '\\n')\n",
    "                    \n",
    "#                 # scheduler hard coding \n",
    "#                 if epoch > (sch_epoch+1):\n",
    "#                     log.write('step scheduler')\n",
    "#                     scheduler.step()\n",
    "#                     sch_epoch +=1\n",
    "\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__module__       : __main__\n",
      "debug            : False\n",
      "val_samples      : 5000\n",
      "iter_log         : 5000\n",
      "amp              : True\n",
      "gpu              : 0\n",
      "iteration        : 80000000\n",
      "batch_size       : 64\n",
      "weight_decay     : 1e-06\n",
      "n_fold           : 5\n",
      "fold             : 3\n",
      "dir_             : ./saved_models/768_decoder_1e-3/\n",
      "initial_checkpoint : ./pretrained/00085000_2.3856_0.0000model.pth\n",
      "start_lr         : 0.001\n",
      "min_lr           : 1e-06\n",
      "vocab_size       : 193\n",
      "max_length       : 300\n",
      "patch_dim        : 768\n",
      "text_dim         : 768\n",
      "num_layer        : 5\n",
      "num_head         : 8\n",
      "ff_dim           : 1024\n",
      "patch_size       : 16\n",
      "pixel_pad        : 3\n",
      "pixel_stride     : 4\n",
      "num_pixel        : 16\n",
      "max_patch_row_col : 500\n",
      "max_num_patch    : 600\n",
      "T_max            : 10\n",
      "opt              : radam_look\n",
      "scheduler        : None\n",
      "loss             : bce\n",
      "factor           : 0.35\n",
      "patience         : 3\n",
      "eps              : 1e-06\n",
      "decay_epoch      : [4, 8, 12, 16]\n",
      "T_0              : 4\n",
      "num_workers      : 16\n",
      "seed             : 92\n",
      "__dict__         : <attribute '__dict__' of 'args' objects>\n",
      "__weakref__      : <attribute '__weakref__' of 'args' objects>\n",
      "__doc__          : None\n",
      "\n",
      "0 fold pass\n",
      "1 fold pass\n",
      "2 fold pass\n",
      "len(tokenizer) : vocab_size 193\n",
      "** dataset setting **\n",
      "train_dataset : \n",
      "\tlen = 2383782\n",
      "\tdf  = (2383782, 12)\n",
      "\tlength distribution\n",
      "\t\t  20 =        0 (0.0000)\n",
      "\t\t  40 =     1467 (0.0006)\n",
      "\t\t  60 =   163512 (0.0686)\n",
      "\t\t  80 =   754403 (0.3165)\n",
      "\t\t 100 =   720141 (0.3021)\n",
      "\t\t 120 =   485601 (0.2037)\n",
      "\t\t 140 =   183232 (0.0769)\n",
      "\t\t 160 =    56892 (0.0239)\n",
      "\t\t 180 =    13030 (0.0055)\n",
      "\t\t 200 =     2777 (0.0012)\n",
      "\t\t 220 =     1402 (0.0006)\n",
      "\t\t 240 =      917 (0.0004)\n",
      "\t\t 260 =      372 (0.0002)\n",
      "\t\t 280 =       36 (0.0000)\n",
      "\n",
      "valid_dataset : \n",
      "\tlen = 40404\n",
      "\tdf  = (40404, 12)\n",
      "\tlength distribution\n",
      "\t\t  20 =        0 (0.0000)\n",
      "\t\t  40 =       23 (0.0006)\n",
      "\t\t  60 =     2768 (0.0685)\n",
      "\t\t  80 =    12788 (0.3165)\n",
      "\t\t 100 =    12207 (0.3021)\n",
      "\t\t 120 =     8231 (0.2037)\n",
      "\t\t 140 =     3108 (0.0769)\n",
      "\t\t 160 =      963 (0.0238)\n",
      "\t\t 180 =      219 (0.0054)\n",
      "\t\t 200 =       48 (0.0012)\n",
      "\t\t 220 =       23 (0.0006)\n",
      "\t\t 240 =       19 (0.0005)\n",
      "\t\t 260 =        6 (0.0001)\n",
      "\t\t 280 =        1 (0.0000)\n",
      "\n",
      "\n",
      "** net setting **\n",
      "\n",
      "scheduler is None** start training here! **\n",
      "                    |----- VALID -----------|---- TRAIN/BATCH --------------\n",
      "rate    iter  epoch | loss  lb(tfO) lb(tfX) | loss0  loss1  | time          \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1780/37246 [07:02<1:58:01,  5.01it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f541e976670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jeonghokim/anaconda3/envs/bms38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/jeonghokim/anaconda3/envs/bms38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1300, in _shutdown_workers\n",
      "    q.close()\n",
      "  File \"/home/jeonghokim/anaconda3/envs/bms38/lib/python3.8/multiprocessing/queues.py\", line 142, in close\n",
      "    close()\n",
      "  File \"/home/jeonghokim/anaconda3/envs/bms38/lib/python3.8/multiprocessing/util.py\", line 226, in __call__\n",
      "    self._kwargs = self._key = None\n",
      "KeyboardInterrupt: \n",
      "  5%|▍         | 1780/37246 [07:03<2:20:34,  4.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9399d1bdb0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# main #################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-84e3f3ed91c6>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    293\u001b[0m                         \u001b[0;31m#assert(False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                         \u001b[0;31m#logit = data_parallel(net, (image, token, length))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_pad_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_pad_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0mloss0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_cross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bms38/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mdata_parallel\u001b[0;34m(module, inputs, device_ids, output_device, dim, module_kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mused_device_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_device_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bms38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7a74611108ee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, patch, coord, token, patch_pad_mask, token_pad_mask)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtext_pad_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_pad_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mimage_pad_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_pad_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_of_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pad_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_pad_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bms38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-42509080cf99>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mem, x_mask, x_pad_mask, mem_pad_mask)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m#print('my TransformerDecode forward()')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 x = layer(\n\u001b[0m\u001b[1;32m     85\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mmem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bms38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bms38/lib/python3.8/site-packages/fairseq/modules/transformer_layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_out, encoder_padding_mask, incremental_state, prev_self_attn_state, prev_attn_state, self_attn_mask, self_attn_padding_mask, need_attn, need_head_weights)\u001b[0m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[1;32m    384\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_attn_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bms38/lib/python3.8/site-packages/fairseq/modules/transformer_layer.py\u001b[0m in \u001b[0;36mresidual_connection\u001b[0;34m(self, x, residual)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresidual_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     def forward(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main #################################################################\n",
    "if __name__ == '__main__':\n",
    "    run_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    debug=False\n",
    "    val_samples = 100 if debug  else 100_000 # 384 images -> 100_000\n",
    "    \n",
    "    amp = True\n",
    "    gpu = '0,1,2,3'\n",
    "    #multi_gpu=True if len(gpu)>1 else False\n",
    "    #encoder='resnet101'#'[resnet34', 'tf_efficientnet_b0']\n",
    "    #decoder='unet'\n",
    "    \n",
    "    #epochs=2 \n",
    "    iteration = 80000 * 1000\n",
    "    batch_size=256\n",
    "    weight_decay=1e-6\n",
    "    n_fold=5\n",
    "    fold=3 # [0, 1, 2, 3, 4]\n",
    "    #all_fold_train = False # all fold training\n",
    "    dir_ = f'./saved_models/trash/'#f'./saved_models/fold3_224_tnt/'\n",
    "    initial_checkpoint = './saved_models/384_to_448_5e-6/checkpoint/00062000_1.2408_1.1114model.pth'\n",
    "    #'./saved_models/scratch_fold3_224_tnt/checkpoint/2epoch_1.4170_2.1683_model.pth'\n",
    "    #'./saved_models/scratch_fold3_224_tnt/checkpoint/5epoch_1.8344model.pth'#'./pretrained/00298000_model.pth'\n",
    "    #encoder_checkpoint = None\n",
    "    start_lr = 1e-4#1e-3,5e-5\n",
    "    min_lr=1e-6\n",
    "    # ---- Dataset ---- #\n",
    "    image_size = 448\n",
    "    vocab_size = 193\n",
    "    max_length = 300 #275\n",
    "    image_path = f'./data/{image_size}x{image_size}/train'\n",
    "\n",
    "    # -------- Encoder --------- #\n",
    "    image_dim = 384 # embedding dimension을 뜻하는듯\n",
    "    text_dim  = 384\n",
    "    decoder_dim = 384\n",
    "    num_layer = 5 # 3\n",
    "    num_head = 8\n",
    "    ff_dim = 1024\n",
    "    #num_pixel=8*8 # when resnet101,256size 8x8, 224-> 7x7\n",
    "    \n",
    "    # 내맘대로 추가 dimension\n",
    "    #in_dim = 40 # 24\n",
    "    #num_heads = 10 #6\n",
    "    # ---- optimizer, scheduler .. ---- #\n",
    "    T_max=6 # CosineAnnealingLR\n",
    "    opt =  'radam_look' # [adamw, radam_look]\n",
    "    scheduler=None #'MultiStepLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    loss = 'bce' # [lovasz, bce, bce_dice, dice]\n",
    "    factor=0.35 # ReduceLROnPlateau, MultiStepLR\n",
    "    patience=3 # ReduceLROnPlateau\n",
    "    eps=1e-6 # ReduceLROnPlateau\n",
    "    \n",
    "    decay_epoch = [4, 8, 12, 16]\n",
    "    T_0=4 # CosineAnnealingWarmRestarts\n",
    "\n",
    "    \n",
    "    #----------------------------------#\n",
    "\n",
    "    # ---- Else ---- #\n",
    "    num_workers=8\n",
    "    seed=92\n",
    "\n",
    "STOI = {\n",
    "    '<sos>': 190,\n",
    "    '<eos>': 191,\n",
    "    '<pad>': 192,\n",
    "}\n",
    "\n",
    "data_dir = './data/'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "##----------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True # for faster training, but not deterministic\n",
    "import random\n",
    "set_seeds(seed=args.seed)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0,
     59,
     93,
     187
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def do_valid(net, tokenizer, valid_loader):\n",
    "\n",
    "    valid_probability = []\n",
    "    valid_truth = []\n",
    "    valid_length = []\n",
    "    valid_num = 0\n",
    "\n",
    "    net.eval()\n",
    "    start_timer = timer()\n",
    "    for t, batch in enumerate(valid_loader):\n",
    "        batch_size = len(batch['index'])\n",
    "        image  = batch['image' ].cuda()\n",
    "        token  = batch['token' ].cuda()\n",
    "        length = batch['length']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = data_parallel(net,(image, token, length)) #net(image, token, length)\n",
    "            probability = F.softmax(logit,-1)\n",
    "\n",
    "        valid_num += batch_size\n",
    "        valid_probability.append(probability.data.cpu().numpy())\n",
    "        valid_truth.append(token.data.cpu().numpy())\n",
    "        valid_length.extend(length)\n",
    "        print('\\r %8d / %d  %s'%(valid_num, len(valid_loader.sampler),time_to_str(timer() - start_timer,'sec')),end='',flush=True)\n",
    "\n",
    "    assert(valid_num == len(valid_loader.sampler)) #len(valid_loader.dataset))\n",
    "    #print('')\n",
    "    #----------------------\n",
    "    probability = np.concatenate(valid_probability)\n",
    "    predict = probability.argmax(-1)\n",
    "    truth   = np.concatenate(valid_truth)\n",
    "    length  = valid_length\n",
    "\n",
    "\n",
    "    #----\n",
    "    p = probability[:,:-1].reshape(-1, args.vocab_size)\n",
    "    t = truth[:,1:].reshape(-1)\n",
    "\n",
    "    non_pad = np.where(t!=STOI['<pad>'])[0] #& (t!=STOI['<sos>'])\n",
    "    p = p[non_pad]\n",
    "    t = t[non_pad]\n",
    "    loss = np_loss_cross_entropy(p, t)\n",
    "\n",
    "    #----\n",
    "    lb_score = 0\n",
    "    if 1:\n",
    "        score = []\n",
    "        for i, (p, t) in enumerate(zip(predict, truth)):\n",
    "            t = truth[i][1:length[i]-1]     # in the buggy version, i have used 1 instead of i\n",
    "            p = predict[i][1:length[i]-1]\n",
    "            t = tokenizer.one_predict_to_inchi(t)\n",
    "            p = tokenizer.one_predict_to_inchi(p)\n",
    "            s = Levenshtein.distance(p, t)\n",
    "            score.append(s)\n",
    "        lb_score = np.mean(score)\n",
    "\n",
    "    #lb_score = compute_lb_score(k, t)\n",
    "    return [loss, lb_score]\n",
    "\n",
    "def do_predict(net, tokenizer, valid_loader):\n",
    "\n",
    "    text = []\n",
    "    tokens = []\n",
    "    inchis = []\n",
    "    start_timer = timer()\n",
    "    valid_num = 0\n",
    "    for t, batch in enumerate(valid_loader):\n",
    "        batch_size = len(batch['image'])\n",
    "        image = batch['image'].cuda()\n",
    "        inchi = batch['InChI']\n",
    "        inchis.extend(inchi)\n",
    "        #token  = batch['token']\n",
    "        #tokens.extend(token)\n",
    "\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            with amp.autocast():\n",
    "                k = net.forward_argmax_decode(image)\n",
    "\n",
    "\n",
    "                k = k.data.cpu().numpy()\n",
    "                k = tokenizer.predict_to_inchi(k)\n",
    "                text.extend(k)\n",
    "\n",
    "        valid_num += batch_size\n",
    "        print('\\r %8d / %d  %s' % (valid_num, len(valid_loader.sampler), time_to_str(timer() - start_timer, 'sec')),\n",
    "              end='', flush=True)\n",
    "\n",
    "    assert(valid_num == len(valid_loader.sampler))\n",
    "    print('')\n",
    "    return text, inchis\n",
    "\n",
    "def run_train():\n",
    "    out_dir = args.dir_\n",
    "\n",
    "\n",
    "    ## setup  ----------------------------------------\n",
    "    for f in ['checkpoint', 'train', 'valid', 'backup']: os.makedirs(out_dir + '/' + f, exist_ok=True)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_dir + '/log.train.txt', mode='a')\n",
    "    print_args(args, log)\n",
    "    log.write('\\n')\n",
    "    \n",
    "    \n",
    "    #\n",
    "    for n_fold in range(5):\n",
    "        if n_fold != args.fold:\n",
    "            print(f'{n_fold} fold pass')\n",
    "            continue\n",
    "    \n",
    "        ## dataset ------------------------------------\n",
    "        df_train, df_valid = make_fold('train-%d' % n_fold)\n",
    "\n",
    "        tokenizer = load_tokenizer()\n",
    "        train_dataset = BmsDataset(df_train,tokenizer)\n",
    "        valid_dataset = BmsDataset(df_valid,tokenizer)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size=args.batch_size,\n",
    "            drop_last=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            collate_fn=null_collate,\n",
    "        )\n",
    "        \n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            #sampler=SequentialSampler(valid_dataset),\n",
    "            sampler=FixNumSampler(valid_dataset, args.val_samples), #200_000 #5_000\n",
    "            batch_size=args.batch_size,\n",
    "            drop_last=False,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            collate_fn=null_collate,\n",
    "            #collate_fn=lambda batch: null_collate(batch,False),\n",
    "        )\n",
    "\n",
    "        log.write('** dataset setting **\\n')\n",
    "        log.write('train_dataset : \\n%s\\n' % (train_dataset))\n",
    "        log.write('valid_dataset : \\n%s\\n' % (valid_dataset))\n",
    "        log.write('\\n')\n",
    "\n",
    "\n",
    "        ## net ----------------------------------------\n",
    "        scaler = amp.GradScaler()\n",
    "        net = Net().to(device)\n",
    "\n",
    "        #if args.encoder_checkpoint is not None:\n",
    "        #    f = torch.load(encoder_checkpoint, map_location=lambda storage, loc: storage)\n",
    "        #    encoder_state_dict = f['encoder_state_dict']\n",
    "        #    net.encoder.load_state_dict(encoder_state_dict, strict=False)  # True\n",
    "\n",
    "        if args.initial_checkpoint is not None:\n",
    "            f = torch.load(args.initial_checkpoint, map_location='cpu')\n",
    "            net.load_state_dict(f['state_dict'], strict=True)  # True\n",
    "            #\n",
    "            #from collections import OrderedDict\n",
    "            #temp_parms = OrderedDict()\n",
    "            #for name_ in f['state_dict']:\n",
    "            #    if name_=='cnn.e.patch_pos':\n",
    "            #        continue\n",
    "            #    temp_parms[name_] = f['state_dict'][name_]\n",
    "            #net.load_state_dict(temp_parms, strict=False)\n",
    "            #\n",
    "\n",
    "        else:\n",
    "            start_iteration = 0\n",
    "            start_epoch = 0\n",
    "\n",
    "        log.write('** net setting **\\n')\n",
    "        log.write('\\n')\n",
    "        # -----------------------------------------------\n",
    "        if 0:  ##freeze\n",
    "            for p in net.encoder.parameters(): p.requires_grad = False\n",
    "\n",
    "        # ------------------------\n",
    "        #  Optimizer\n",
    "        # ------------------------\n",
    "        optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad,net.parameters()), lr=args.start_lr), alpha=0.5, k=5)\n",
    "\n",
    "        # ------------------------\n",
    "        #  scheduler\n",
    "        # ------------------------\n",
    "        def get_scheduler(optimizer):\n",
    "            if args.scheduler=='ReduceLROnPlateau':\n",
    "                scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=args.factor, patience=args.patience, \n",
    "                                              min_lr = 1e-5, verbose=True, eps=args.eps)\n",
    "            elif args.scheduler=='CosineAnnealingLR':\n",
    "                scheduler = CosineAnnealingLR(optimizer, T_max=args.T_max, eta_min=args.min_lr, last_epoch=-1)\n",
    "            elif args.scheduler=='CosineAnnealingWarmRestarts':\n",
    "                scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=args.T_0, T_mult=1, eta_min=args.min_lr, last_epoch=-1)\n",
    "            elif args.scheduler == 'MultiStepLR':\n",
    "                scheduler = MultiStepLR(optimizer, milestones=args.decay_epoch, gamma= args.factor, verbose=True)\n",
    "            elif args.scheduler == 'OneCycleLR':\n",
    "                scheduler = OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=1e-3, epochs=args.epochs, steps_per_epoch=len(train_loader))\n",
    "            else:\n",
    "                scheduler = None\n",
    "                log.write('scheduler is None')\n",
    "            return scheduler\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "\n",
    "        ## start training here! ##############################################\n",
    "        log.write('** start training here! **\\n')\n",
    "        log.write('                    |----- VALID -----------|---- TRAIN/BATCH --------------\\n')\n",
    "        log.write('rate    iter  epoch | loss  lb(tfO) lb(tfX) | loss0  loss1  | time          \\n')\n",
    "        log.write('----------------------------------------------------------------------------\\n')\n",
    "                 # 0.00000   0.00* 0.00  | 0.000  0.000  | 0.000  0.000  |  0 hr 00 min\n",
    "\n",
    "        num_iteration = args.iteration\n",
    "        iter_log = 1000\n",
    "        iter_valid = 1000\n",
    "        iter_save = list(range(0, num_iteration, 1000))  # 1*1000\n",
    "        def message(mode='print'):\n",
    "            if mode == ('print'):\n",
    "                asterisk = ' '\n",
    "                loss = batch_loss\n",
    "            if mode == ('log'):\n",
    "                asterisk = '*' if iteration in iter_save else ' '\n",
    "                loss = train_loss\n",
    "\n",
    "            text = \\\n",
    "                '%0.5f  %5.4f%s %4.2f  | ' % (rate, iteration / 10000, asterisk, epoch,) + \\\n",
    "                '%4.3f  %4.3f  %4.3f   | ' % (valid_loss[0],valid_loss[1],tfX_score) + \\\n",
    "                '%4.3f  %4.3f  %4.3f   | ' % (*loss,) + \\\n",
    "                '%s' % (time_to_str(timer() - start_timer, 'min'))\n",
    "\n",
    "            return text\n",
    "\n",
    "        # ----\n",
    "        valid_loss = np.zeros(2, np.float32)\n",
    "        train_loss = np.zeros(3, np.float32)\n",
    "        batch_loss = np.zeros_like(train_loss)\n",
    "        sum_train_loss = np.zeros_like(train_loss)\n",
    "        sum_train = 0\n",
    "        loss0 = torch.FloatTensor([0]).cuda().sum()\n",
    "        loss1 = torch.FloatTensor([0]).cuda().sum()\n",
    "        loss2 = torch.FloatTensor([0]).cuda().sum()\n",
    "\n",
    "        start_timer = timer()\n",
    "        start_iteration = 0 ; start_epoch = 0\n",
    "        rate = 0\n",
    "        iteration = start_iteration\n",
    "        epoch = start_epoch\n",
    "        best_lb = np.inf\n",
    "        \n",
    "\n",
    "        # validation\n",
    "        #valid_loss = do_valid(net, tokenizer, valid_loader)\n",
    "        predict_, truth_ = do_predict(net, tokenizer, valid_loader)\n",
    "        tfX_score = compute_lb_score(predict_, truth_).mean()\n",
    "        log.write('\\n'+message(mode='log') + '\\n')\n",
    "        \n",
    "        return predict_, truth_\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__module__       : __main__\n",
      "debug            : False\n",
      "val_samples      : 100000\n",
      "amp              : True\n",
      "gpu              : 0,1,2,3\n",
      "iteration        : 80000000\n",
      "batch_size       : 256\n",
      "weight_decay     : 1e-06\n",
      "n_fold           : 5\n",
      "fold             : 3\n",
      "dir_             : ./saved_models/trash/\n",
      "initial_checkpoint : ./saved_models/384_to_448_5e-6/checkpoint/00062000_1.2408_1.1114model.pth\n",
      "start_lr         : 0.0001\n",
      "min_lr           : 1e-06\n",
      "image_size       : 448\n",
      "vocab_size       : 193\n",
      "max_length       : 300\n",
      "image_path       : ./data/448x448/train\n",
      "image_dim        : 384\n",
      "text_dim         : 384\n",
      "decoder_dim      : 384\n",
      "num_layer        : 5\n",
      "num_head         : 8\n",
      "ff_dim           : 1024\n",
      "T_max            : 6\n",
      "opt              : radam_look\n",
      "scheduler        : None\n",
      "loss             : bce\n",
      "factor           : 0.35\n",
      "patience         : 3\n",
      "eps              : 1e-06\n",
      "decay_epoch      : [4, 8, 12, 16]\n",
      "T_0              : 4\n",
      "num_workers      : 8\n",
      "seed             : 92\n",
      "__dict__         : <attribute '__dict__' of 'args' objects>\n",
      "__weakref__      : <attribute '__weakref__' of 'args' objects>\n",
      "__doc__          : None\n",
      "\n",
      "0 fold pass\n",
      "1 fold pass\n",
      "2 fold pass\n",
      "len(tokenizer) : vocab_size 193\n",
      "** dataset setting **\n",
      "train_dataset : \n",
      "\tlen = 2020155\n",
      "\tdf  = (2020155, 9)\n",
      "\n",
      "valid_dataset : \n",
      "\tlen = 404031\n",
      "\tdf  = (404031, 9)\n",
      "\n",
      "\n",
      "** net setting **\n",
      "\n",
      "scheduler is None** start training here! **\n",
      "                    |----- VALID -----------|---- TRAIN/BATCH --------------\n",
      "rate    iter  epoch | loss  lb(tfO) lb(tfX) | loss0  loss1  | time          \n",
      "----------------------------------------------------------------------------\n",
      "   100000 / 100000  30 min 45 sec\n",
      "\n",
      "0.00000  0.0000* 0.00  | 0.000  0.000  1.188   | 0.000  0.000  0.000   |  0 hr 30 min\n"
     ]
    }
   ],
   "source": [
    "# main #################################################################\n",
    "if __name__ == '__main__':\n",
    "    predict, truth = run_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## validation 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rdkit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-58a77840780b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrdkit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rdkit'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import rdkit\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#predict = np.load('./data/predict_1.1114.npy')\n",
    "#truth = np.load('./data/truth_1.1114.npy')\n",
    "df = pd.read_csv('./data/image_ratio.csv')\n",
    "df2 = pd.read_csv('./data/train_labels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_ = {}\n",
    "for i, j in zip(predict, truth):\n",
    "    dict_[j] = i\n",
    "\n",
    "df2['pred'] = df2['InChI'].apply(lambda x : dict_.get(x))\n",
    "\n",
    "# validation 예측값만 고르기\n",
    "df3 = df2[~df2['pred'].isnull()].reset_index(drop=True)\n",
    "# 점수 예측\n",
    "df3['score'] = compute_lb_score(df3['InChI'].values, df3['pred'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df3['score'] = compute_lb_score(df3['InChI'].values, df3['pred'].values)\n",
    "#df3.loc[df3['score']>50, 'score'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "      <th>pred</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00006b137aef</td>\n",
       "      <td>InChI=1S/C22H25NO5S/c1-29(26,27)20-14-12-17(13...</td>\n",
       "      <td>InChI=1S/C22H25NO5S/c1-29(26,27)20-14-12-17(13...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000077bfb356</td>\n",
       "      <td>InChI=1S/C16H23NO3/c1-4-17(5-2)14(18)11-12(3)1...</td>\n",
       "      <td>InChI=1S/C16H23NO3/c1-4-17(5-2)14(18)11-12(3)1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000093a52a28</td>\n",
       "      <td>InChI=1S/C13H19N3O4/c17-12(18)6-16-11(14-13(15...</td>\n",
       "      <td>InChI=1S/C13H19N3O4/c17-12(18)6-16-11(14-13(15...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000a651dd85</td>\n",
       "      <td>InChI=1S/C29H24N4O4S/c1-36-23-10-7-20(31-27(35...</td>\n",
       "      <td>InChI=1S/C29H24N4O4S/c1-36-23-10-7-20(31-27(35...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000d93e0e9b</td>\n",
       "      <td>InChI=1S/C31H40FNO4/c1-30-16-13-23(34)19-21(30...</td>\n",
       "      <td>InChI=1S/C31H40FNO4/c1-30-16-13-23(34)19-21(30...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>3f7255cbda17</td>\n",
       "      <td>InChI=1S/C22H22FN3O2/c23-17-7-2-8-18-19(17)22(...</td>\n",
       "      <td>InChI=1S/C22H22FN3O2/c23-17-7-2-8-18-19(17)22(...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>3f727de87ea9</td>\n",
       "      <td>InChI=1S/C10H14N4O/c1-6-3-4-8(7(2)5-6)13-9(11)...</td>\n",
       "      <td>InChI=1S/C10H14N4O/c1-6-3-4-8(7(2)5-6)13-9(11)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>3f72aaca9340</td>\n",
       "      <td>InChI=1S/C30H27FN10O2/c1-17-15-40(25-8-7-22(31...</td>\n",
       "      <td>InChI=1S/C30H27FN10O2/c1-17-15-40(25-8-7-22(31...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>3f72ca8276ae</td>\n",
       "      <td>InChI=1S/C16H14N2O3S2/c1-21-13(19)10-23-16-17-...</td>\n",
       "      <td>InChI=1S/C16H14N2O3S2/c1-21-13(19)10-23-16-17-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>3f72cbecf4a8</td>\n",
       "      <td>InChI=1S/C10H10Cl2O2/c1-5-4-7(14-3)8(10(12)13)...</td>\n",
       "      <td>InChI=1S/C10H10Cl2O2/c1-5-4-7(14-3)8(10(12)13)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_id                                              InChI  \\\n",
       "0      00006b137aef  InChI=1S/C22H25NO5S/c1-29(26,27)20-14-12-17(13...   \n",
       "1      000077bfb356  InChI=1S/C16H23NO3/c1-4-17(5-2)14(18)11-12(3)1...   \n",
       "2      000093a52a28  InChI=1S/C13H19N3O4/c17-12(18)6-16-11(14-13(15...   \n",
       "3      0000a651dd85  InChI=1S/C29H24N4O4S/c1-36-23-10-7-20(31-27(35...   \n",
       "4      0000d93e0e9b  InChI=1S/C31H40FNO4/c1-30-16-13-23(34)19-21(30...   \n",
       "...             ...                                                ...   \n",
       "99995  3f7255cbda17  InChI=1S/C22H22FN3O2/c23-17-7-2-8-18-19(17)22(...   \n",
       "99996  3f727de87ea9  InChI=1S/C10H14N4O/c1-6-3-4-8(7(2)5-6)13-9(11)...   \n",
       "99997  3f72aaca9340  InChI=1S/C30H27FN10O2/c1-17-15-40(25-8-7-22(31...   \n",
       "99998  3f72ca8276ae  InChI=1S/C16H14N2O3S2/c1-21-13(19)10-23-16-17-...   \n",
       "99999  3f72cbecf4a8  InChI=1S/C10H10Cl2O2/c1-5-4-7(14-3)8(10(12)13)...   \n",
       "\n",
       "                                                    pred  score  \n",
       "0      InChI=1S/C22H25NO5S/c1-29(26,27)20-14-12-17(13...      0  \n",
       "1      InChI=1S/C16H23NO3/c1-4-17(5-2)14(18)11-12(3)1...      1  \n",
       "2      InChI=1S/C13H19N3O4/c17-12(18)6-16-11(14-13(15...      1  \n",
       "3      InChI=1S/C29H24N4O4S/c1-36-23-10-7-20(31-27(35...      0  \n",
       "4      InChI=1S/C31H40FNO4/c1-30-16-13-23(34)19-21(30...      0  \n",
       "...                                                  ...    ...  \n",
       "99995  InChI=1S/C22H22FN3O2/c23-17-7-2-8-18-19(17)22(...      0  \n",
       "99996  InChI=1S/C10H14N4O/c1-6-3-4-8(7(2)5-6)13-9(11)...      0  \n",
       "99997  InChI=1S/C30H27FN10O2/c1-17-15-40(25-8-7-22(31...     57  \n",
       "99998  InChI=1S/C16H14N2O3S2/c1-21-13(19)10-23-16-17-...      0  \n",
       "99999  InChI=1S/C10H10Cl2O2/c1-5-4-7(14-3)8(10(12)13)...      0  \n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "      <th>pred</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0026c325777c</td>\n",
       "      <td>InChI=1S/C20H19NO3/c1-11-13-10-14(12-8-6-5-7-9...</td>\n",
       "      <td>InChI=1S/C19H17NO3/c1-18(2)15-13(16(21)23-17(1...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>004560fcae72</td>\n",
       "      <td>InChI=1S/C12H13N7O/c1-8(11-16-18-19-17-11)15-1...</td>\n",
       "      <td>InChI=1S/C11H11N7O/c12-5-1-2-8-3-4-9(13-6-8)11...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>00471d7a3f03</td>\n",
       "      <td>InChI=1S/C20H24N6/c1-13-10-19-18(25(13)12-15-6...</td>\n",
       "      <td>InChI=1S/C19H22N6/c1-13-22-23-19-15(16(21)7-9-...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>006506d4fe30</td>\n",
       "      <td>InChI=1S/C31H40N6O5Si/c1-32-29(38)22-6-7-25(26...</td>\n",
       "      <td>InChI=1S/C32H42N6O5Si/c1-33-29(39)22-7-8-26(27...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>00863bc912bc</td>\n",
       "      <td>InChI=1S/C33H41N3O5S/c1-4-31(33(38)34-27-14-7-...</td>\n",
       "      <td>InChI=1S/C31H36FN3O5S/c1-23(31(37)33-26-12-5-3...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98020</th>\n",
       "      <td>3e2fff16d7af</td>\n",
       "      <td>InChI=1S/C27H29Cl2N3O5S/c1-4-37-25-16-9-8-15-2...</td>\n",
       "      <td>InChI=1S/C26H27Cl2N3O5S/c1-18(26(33)29-2)30(16...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98757</th>\n",
       "      <td>3ea55bddc868</td>\n",
       "      <td>InChI=1S/C28H44N2/c1-2-5-9-13-17-29-21-25-19-2...</td>\n",
       "      <td>InChI=1S/C27H44N2/c1-2-6-10-16-28-20-24-18-23-...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99394</th>\n",
       "      <td>3f0c536ef0fa</td>\n",
       "      <td>InChI=1S/C30H37N3O3/c34-28(31-21-22-13-15-24(1...</td>\n",
       "      <td>InChI=1S/C30H37N3O3/c34-28(25-10-4-5-11-26(25)...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99835</th>\n",
       "      <td>3f5656899b3b</td>\n",
       "      <td>InChI=1S/C42H22N4/c1-2-6-32-31(5-1)36-16-28-12...</td>\n",
       "      <td>InChI=1S/C42H22N4/c1-2-6-32-31(5-1)37-15-27-11...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>3f72aaca9340</td>\n",
       "      <td>InChI=1S/C30H27FN10O2/c1-17-15-40(25-8-7-22(31...</td>\n",
       "      <td>InChI=1S/C30H27FN10O2/c1-17-15-40(25-8-7-22(31...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_id                                              InChI  \\\n",
       "263    0026c325777c  InChI=1S/C20H19NO3/c1-11-13-10-14(12-8-6-5-7-9...   \n",
       "450    004560fcae72  InChI=1S/C12H13N7O/c1-8(11-16-18-19-17-11)15-1...   \n",
       "458    00471d7a3f03  InChI=1S/C20H24N6/c1-13-10-19-18(25(13)12-15-6...   \n",
       "635    006506d4fe30  InChI=1S/C31H40N6O5Si/c1-32-29(38)22-6-7-25(26...   \n",
       "840    00863bc912bc  InChI=1S/C33H41N3O5S/c1-4-31(33(38)34-27-14-7-...   \n",
       "...             ...                                                ...   \n",
       "98020  3e2fff16d7af  InChI=1S/C27H29Cl2N3O5S/c1-4-37-25-16-9-8-15-2...   \n",
       "98757  3ea55bddc868  InChI=1S/C28H44N2/c1-2-5-9-13-17-29-21-25-19-2...   \n",
       "99394  3f0c536ef0fa  InChI=1S/C30H37N3O3/c34-28(31-21-22-13-15-24(1...   \n",
       "99835  3f5656899b3b  InChI=1S/C42H22N4/c1-2-6-32-31(5-1)36-16-28-12...   \n",
       "99997  3f72aaca9340  InChI=1S/C30H27FN10O2/c1-17-15-40(25-8-7-22(31...   \n",
       "\n",
       "                                                    pred  score  \n",
       "263    InChI=1S/C19H17NO3/c1-18(2)15-13(16(21)23-17(1...     57  \n",
       "450    InChI=1S/C11H11N7O/c12-5-1-2-8-3-4-9(13-6-8)11...     56  \n",
       "458    InChI=1S/C19H22N6/c1-13-22-23-19-15(16(21)7-9-...     65  \n",
       "635    InChI=1S/C32H42N6O5Si/c1-33-29(39)22-7-8-26(27...     59  \n",
       "840    InChI=1S/C31H36FN3O5S/c1-23(31(37)33-26-12-5-3...     76  \n",
       "...                                                  ...    ...  \n",
       "98020  InChI=1S/C26H27Cl2N3O5S/c1-18(26(33)29-2)30(16...     92  \n",
       "98757  InChI=1S/C27H44N2/c1-2-6-10-16-28-20-24-18-23-...     77  \n",
       "99394  InChI=1S/C30H37N3O3/c34-28(25-10-4-5-11-26(25)...     76  \n",
       "99835  InChI=1S/C42H22N4/c1-2-6-32-31(5-1)37-15-27-11...     77  \n",
       "99997  InChI=1S/C30H27FN10O2/c1-17-15-40(25-8-7-22(31...     57  \n",
       "\n",
       "[603 rows x 4 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df3.loc[df3['score']>50]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 450\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=450x193 at 0x7EFB13B470D0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = random.sample(temp['image_id'].tolist(), 1)[0]\n",
    "img2 = Image.open(f'./data/train/{img[0]}/{img[1]}/{img[2]}/{img}.png')\n",
    "print(img2.height, img2.width)\n",
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 330\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAACcCAAAAAA8Ur1fAAAE1UlEQVR4nO2d23arOgxFpTP2//+yzwOXQIpBgmUsxWs+dHS0xrFn5QtGSbUIwfBf7wb8DlQJgyphUCUMqoRBlTCoEgZVwqBKGFQJgyphUCUMqoRBlTCoEgZVwqBKGFQJgyphUCUMqoRBlTCoEgZVwqBKGHFUqqj2bsMjNFSiS6zWOAkTlbkjUiSQShERKZmFxlLJAU5EAqksmn2+TD2kYhEmKvPzXGXuUQnkuUrOEDNxBrhK8giPozI9VAmDmyEYcaKScyVZoEoYVAmDKmHEUpl6OxFLJVdwIkKVQOKoTD1PikRSmXqeFImkUiR5ZPpVtoye1JF5IypdjwY9eUCpRd5RWcQxELWYEy6KmotqyFQt/3mlFvtV9trV8eeZijqueIc7y445eNbOXl6hxRPoIiJij/eXuKPS1utt1JTz8bgLsIuiaq71bW4N8IuxdTgF1F/oT2316h1F3+feXDl/V+/HQbW10kctqLRq++Pl+zgynz0mO+5HtXdHgivSDqr4KroWifKk72k7Dvp8UuV36bO4vi66vlAMmb5WHA7GfR9xA243nI9rDTXKXSp3hTfN3+56DPWpGLemhjIaZ5R7mnC2fk712Gqzx9BU8rR8nCnTpfJsWVYp9nsgLdYX1utalzb0HuUOlcdFNzJdVZm9OwT1Dcx/5pKVdtqlbGoqdkHgKaMh5hvHurAy32P7egO951tuy5F1ujGrPG3mfFRjpzTpdt9b8u7r3u9Qj0qdvig+WU/XL64rYMUaYRjgKfoRgGuVnAGM1DdDRWVaHsxbbzOtArjvX72uUsvyBd7C6W4QT9819HKAQ1edTUW/N2+Y9pWwbu+OzX4N3JgYfoeKyxlqaTLFfixW+tVDktyDX9E0IswHcj2xqux8UtD11Y1YVfYdO8ZXTzLArwIjwvrNQ7Yfwb7sJJivciw75x/yFURzkrnynFYufQfPWaLyIhGwVUdstUYYFk+jcs4WbTa0bM9uNdUTx7+oqC6PTdt9XKKh4iWlunMS8E2VqipFtgnk+LN2a1M+eS6l60C3Z2eITM2uHqw3cnkell8JQzdyRWD4VB7l4vacoQ7S2Bw5NOjGOFNV9/mjL870pynr+192Cky/Sl2yWl5u8N/szmrKb5/AdCdQ9xzQhpRq02/bkOoUfbMCrRugSskOb49yq+y6Df4Iusyafv99e+kO2bZD96LxL4/ydCpdb4p4NU09ocqp0U47L/Qzpcpbuf7Ne5pTpYu33tkzgMrNlNm0s6n2lXdZNkZtT45+XuUssfURtQygcgnIdcfe7DZohLnyew1vtMscQeXfNbyJzCFUHtz2NOj3ICo/tNsY/fyys7Ks4csjUs9lanl3wzgqn6zhJu/jqBQps0TfubD5n6Y5nzgmZ3m2e+O5yrXNkaJS5PNMw3HIXuZQvnI/msp1lHs+HM/2eHU4lZvDDavMTTyeXTDcvnLPafcdn9F3WdcIVAW4zQw4wPdUzjDVH2PDR6UcZsrcsTJ2VH6fZS4/vhVfjEoR+UpUuHkCR5Uzz7MPqHLl82EhD64nE88O16kSxtgrOBSqhEGVMKgSBlXCoEoYVAmDKmFQJQyqhEGVMKgSBlXCoEoYVAmDKmFQJQyqhEGVMOAqI3w6Wh/gKsd97MYBDoMqYVAlDKqE8T/UCFckhuDw+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=330x156 at 0x7EFB13B69550>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = '004560fcae72'#'152af63fb0d0'#'fe214778b4cb'\n",
    "img2 = Image.open(f'./data/train/{img[0]}/{img[1]}/{img[2]}/{img}.png')\n",
    "print(img2.height, img2.width)\n",
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4369it [1:07:11,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "size_=448\n",
    "path = '../bms/data/train/'\n",
    "save_path = f'../bms/data/{size_}x{size_}/train/'\n",
    "a=[]\n",
    "for dirname, _, filenames in tqdm.tqdm(os.walk(path)):#4369\n",
    "    for filename in filenames:\n",
    "        img_path = os.path.join(dirname, filename)\n",
    "        image = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, dsize=(size_,size_), interpolation=cv2.INTER_LINEAR)#INTER_AREA, INTER_LINEAR\n",
    "        file_path = save_path + filename\n",
    "        if not os.path.exists(save_path): \n",
    "            os.makedirs(save_path)\n",
    "        cv2.imwrite(file_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4369it [59:51,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "\n",
    "size_ = 512\n",
    "path = '../bms/data/test/'\n",
    "save_path = f'../bms/data/{size_}x{size_}/test/'\n",
    "a=[]\n",
    "for dirname, _, filenames in tqdm.tqdm(os.walk(path)):#4369\n",
    "    for filename in filenames:\n",
    "        img_path = os.path.join(dirname, filename)\n",
    "        image = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, dsize=(size_,size_), interpolation=cv2.INTER_LINEAR)\n",
    "        file_path = save_path + filename\n",
    "        if not os.path.exists(save_path): \n",
    "            os.makedirs(save_path)\n",
    "        cv2.imwrite(file_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bms38",
   "language": "python",
   "name": "bms38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
